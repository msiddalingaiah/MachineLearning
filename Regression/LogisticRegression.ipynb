{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "[Logisitic regression](https://en.wikipedia.org/wiki/Logistic_regression) is a statistical model used to predict a binary dependent variable, not unlike a binary classifier. Logistic regression is similar to a [single layer perceptron](https://en.wikipedia.org/wiki/Feedforward_neural_network#Single-layer_perceptron), which is the basis for feedforward [artifical neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network).\n",
    "\n",
    "The mathematical definition of logistic regression is presented below, as well as the basics of the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm used for training. Snippets of the equivalent Python code using numpy is also available.\n",
    "\n",
    "A network of single layer perceptrons can be combined to build a [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) for [deep learning](https://en.wikipedia.org/wiki/Deep_learning). More sophisticated structures, such [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs), [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) (RNNs), or [autoencoders](https://en.wikipedia.org/wiki/Autoencoder) can also be assembled as a network of single layer perceptrons.\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "Given a weight vector $W$, bias scalar $b$, and activation function $\\sigma$, activation $a$ describing forward propagation is given by:\n",
    "\n",
    "$$ a = \\sigma (W^T x + b) = (a_1, a_2, ..., a_{m-1}, a_m) \\tag{1}$$\n",
    "\n",
    "The equivalent Python code is:\n",
    "\n",
    "`a = sigmoid(np.dot(W.T, x) + b)`\n",
    "\n",
    "Other activation functions, such as a [rectified linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) (ReLU) or [softmax](https://en.wikipedia.org/wiki/Softmax_function) can also be used in place of [sigmoid functions](https://en.wikipedia.org/wiki/Sigmoid_function). ReLU does not suffer from the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) as found in the traditional sigmoid function. For this reason, ReLU is preferred in hidden layers of deep neural networks. Softmax is often used at the output layer of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "Training through backpropagation is an iterative process of computing the vector $W$ and bias $b$. Training attempts to minimize _loss_ or the _cost function_ given by $J$:\n",
    "\n",
    "$$ J = -\\frac{1}{m} \\sum_{i=1}^{m} y_i log(a_i) + (1 - y_i) log(1 - a_i)\\tag{2}$$\n",
    "\n",
    "`j = (-1/m)*(y.dot(np.log(a.T)) + (1-y).dot(np.log(1 - a.T)))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "Gradients are partial derivatives used to iteratively update weights and biases. The _learning rate_ $\\alpha$ is a hyperparameter tuned for optimal training. This the most basic form of backpropagation. More sophisticated algorithms, such as [Adaptive Moment Estimation](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam) (ADAM) are more efficient.\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m} x \\cdot (a - y)^T \\tag{3}$$\n",
    "\n",
    "`dw = (1/m) * np.dot(x, (a - y).T)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (a_i - y_i) \\tag{4}$$\n",
    "\n",
    "`db = (1/m) * np.sum(a - y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W_{n+1} = W_n - \\alpha \\frac{\\partial J}{\\partial w}\\tag{5}$$\n",
    "\n",
    "`w = w - learning_rate * dw`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ b_{n+1} = b_n - \\alpha \\frac{\\partial J}{\\partial b}\\tag{6}$$\n",
    "\n",
    "`b = b - learning_rate * db`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Derivation\n",
    "\n",
    "To compute the gradient, it is necessary to compute the derivative of the loss function. In simplified form, it looks like this:\n",
    "\n",
    "$$ \\mathcal L = -(y log(a) + (1 - y) log(1 - a))$$\n",
    "\n",
    "$$ a = \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "To find the derivative of $\\mathcal L$ with respect to $z$, we can use the chain rule:\n",
    "\n",
    "$$ \\frac{dJ}{dz} = \\frac{d\\mathcal L}{da} \\frac{da}{dz}$$\n",
    "\n",
    "Using basic derivative rules, we can find these derivatives. It is based on known derivatives of elementary functions and algebraic substitutions. The first term is:\n",
    "\n",
    "$$ \\frac{d\\mathcal L}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$$\n",
    "\n",
    "The second term is:\n",
    "\n",
    "$$ \\frac{da}{dz} = a (1 - a)$$\n",
    "\n",
    "Then substituting above:\n",
    "\n",
    "$$ \\frac{d\\mathcal L}{dz} = \\left(-\\frac{y}{a} + \\frac{1-y}{1-a} \\right) (a (1 - a))$$\n",
    "\n",
    "Then simplifying:\n",
    "\n",
    "$$ \\frac{d\\mathcal L}{dz} = -y(1-a) + a(1-y)$$\n",
    "\n",
    "This produces a surprisingly simple result:\n",
    "\n",
    "$$ \\frac{d\\mathcal L}{dz} = a - y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
